1. dockerfile pull
docker pull dlfrb2002/hadoop_ha:2;

2. swap memory 설정
sudo dd if=/dev/zero of=/var/swapfile bs=1024 count=8000000
sudo mkswap /var/swapfile
sudo chmod 0600 /var/swapfile
sudo swapon /var/swapfile

# 재부팅시에도 swapfile 생성
echo "swapon /var/swapfile" >> /etc/rc.local

3.docker network 생성
$ docker network create --subnet 192.168.31.0/24 hadoop-network



4. docker run
#docker run -it  --name  dlfrb2002/hadoop_ha:2 /bin/bash

docker run -dit --privileged -p 50070:50070 --name hadoop1 --network hadoop-network  --ip 192.168.31.2 --add-host=hadoop1:192.168.31.2 --add-host=hadoop2:192.168.31.3 --add-host=hadoop3:192.168.31.4  --add-host=hadoop4:192.168.31.5 --add-host=hadoop5:192.168.31.6 dlfrb2002/hadoop_ha:2 /bin/bash;

docker run -dit --privileged  --name hadoop2 --network hadoop-network  --ip 192.168.31.3 --add-host=hadoop1:192.168.31.2 --add-host=hadoop2:192.168.31.3 --add-host=hadoop3:192.168.31.4  --add-host=hadoop4:192.168.31.5 --add-host=hadoop5:192.168.31.6 dlfrb2002/hadoop_ha:2 /bin/bash;

docker run -dit --privileged  --name hadoop3 --network hadoop-network  --ip 192.168.31.4 --add-host=hadoop1:192.168.31.2 --add-host=hadoop2:192.168.31.3 --add-host=hadoop3:192.168.31.4  --add-host=hadoop4:192.168.31.5 --add-host=hadoop5:192.168.31.6 dlfrb2002/hadoop_ha:2 /bin/bash;

docker run -dit --privileged  --name hadoop4 --network hadoop-network  --ip 192.168.31.5 --add-host=hadoop1:192.168.31.2 --add-host=hadoop2:192.168.31.3 --add-host=hadoop3:192.168.31.4  --add-host=hadoop4:192.168.31.5 --add-host=hadoop5:192.168.31.6 dlfrb2002/hadoop_ha:2 /bin/bash;

docker run -dit --privileged  --name hadoop5 --network hadoop-network  --ip 192.168.31.6 --add-host=hadoop1:192.168.31.2 --add-host=hadoop2:192.168.31.3 --add-host=hadoop3:192.168.31.4  --add-host=hadoop4:192.168.31.5 --add-host=hadoop5:192.168.31.6 dlfrb2002/hadoop_ha:2 /bin/bash;

passwd root


--ssh_hostkey 생성
$ ssh-keygen -f /etc/ssh/ssh_host_rsa_key -N '' -t rsa; 
$ ssh-keygen -f /etc/ssh/ssh_host_ecdsa_key -N '' -t ecdsa; 
$ ssh-keygen -f /etc/ssh/ssh_host_ed25519_key -N '' -t ed25519;


(one line)
ssh-keygen -f /etc/ssh/ssh_host_rsa_key -N '' -t rsa; ssh-keygen -f /etc/ssh/ssh_host_ecdsa_key -N '' -t ecdsa; ssh-keygen -f /etc/ssh/ssh_host_ed25519_key -N '' -t ed25519;


--ssh 공개키 설정
$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa;
$ touch ~/.ssh/authorized_keys;
$ chmod 0600 ~/.ssh/authorized_keys;

(one line)
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa; touch ~/.ssh/authorized_keys; chmod 0600 ~/.ssh/authorized_keys;

-- ssh daemon 기동
/usr/sbin/sshd

-- active namenode  ssh_key 공개키 배포
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop1;
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop2;
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop3;
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop4;
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop5;


(one line)
ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop1; ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop2; ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop3; ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop4; ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop5;



-- standby namenode 도 ssh key 배포 
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop1;
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop2;
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop3;
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop4;
$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop5;


ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop1; ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop2; ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop3; ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop4; ssh-copy-id -i ~/.ssh/id_rsa.pub root@hadoop5;



-- active /standby namenode 에서  ssh 접속확인
ssh root@hadoop3

--zookeeper myid 파일 변경(각 서버별로..)

echo 1  > /home/hes/data/zookeeper/myid;
echo 2  > /home/hes/data/zookeeper/myid;
echo 3  > /home/hes/data/zookeeper/myid;
echo 4  > /home/hes/data/zookeeper/myid;
echo 5  > /home/hes/data/zookeeper/myid;



-- 
// 이제 server01~05 에서 zookeeper 서버 실행
/home/hes/zookeeper-3.4.8/bin/zkServer.sh start;




-- server01~05에서 zookeeper status 확인
/home/hes/zookeeper-3.4.8/bin/zkServer.sh status


-- zookeeper 초기화(namenode 에서 실행)

/home/hes/hadoop-2.7.3/bin/hdfs zkfc -formatZK

/home/hes/zookeeper-3.4.8/bin/zkCli.sh


// /hadoop-ha 아래에 dfs.nameservices 지정한 nameserviceID 노드가 있으면 성공.
ls /hadoop-ha
[my-hadoop-cluster]
// 빠져나오자
[zk: localhost:2181(CONNECTED) 1] quit


// server01~05 에서 JournalNode 실행

/home/hes/hadoop-2.7.3/sbin/hadoop-daemon.sh start journalnode


//active namenode format
/home/hes/hadoop-2.7.3/bin/hdfs namenode -format;


// 저널 초기화(1번에서)

/home/hes/hadoop-2.7.3/bin/hdfs namenode -initializeSharedEdits


// active NameNode 실행

/home/hes/hadoop-2.7.3/sbin/hadoop-daemon.sh start namenode


// active NameNode용 ZKFC 실행
/home/hes/hadoop-2.7.3/sbin/hadoop-daemon.sh start zkfc



// 확인

[root@server01]# jps

1096 NameNode

1650 DFSZKFailoverController

1456 JournalNode

25051 QuorumPeerMain



// 전체 DataNode 실행 (server01 ~ 05)

/home/hes/hadoop-2.7.3/sbin/hadoop-daemon.sh start datanode




// server02에서 standby NameNode 준비

/home/hes/hadoop-2.7.3/bin/hdfs namenode -bootstrapStandby



// standby NameNode 실행

/home/hes/hadoop-2.7.3/sbin/hadoop-daemon.sh start namenode



// standby NameNode용 ZKFC 실행

/home/hes/hadoop-2.7.3/sbin/hadoop-daemon.sh start zkfc


--active/ standby 확인
/home/hes/hadoop-2.7.3//bin/hdfs haadmin -getServiceState nn1


/home/hes/hadoop-2.7.3//bin/hdfs haadmin -getServiceState nn2


--hadoop 실행(일단 보류)
/home/hes/hadoop-2.7.3/sbin/start-dfs.sh




-- HA 세팅전

로그삭제
공개키 삭제
/etc/sshd 아래 공개키 삭제
/etc/hosts 내용 삭제
/hadoop datafile 삭제